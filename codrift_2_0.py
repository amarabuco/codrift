# -*- coding: utf-8 -*-
"""codrift 2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cJIVkhd_DUoejo_GFpoqveGz4RHePFCy

# Installs
"""

!pip install -U sklearn

!pip install pmdarima

!pip install river

!pip install tslearn

!pip install arch

!pip install skorch

"""# Imports"""

import json
import math
import calendar
from datetime import timedelta 
from datetime import datetime as dt
import numpy as np
import pandas as pd
import warnings
import os
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

import ipywidgets as widgets

#models

#from pmdarima.arima import AutoARIMA as pmautoarima
from pmdarima.arima import auto_arima as pmautoarima
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX as SARIMA

import statsmodels.api as sm
import statsmodels.graphics as smg
import statsmodels.stats as sm_stats
import statsmodels.tsa.api as tsa

from river import evaluate
from river import linear_model
from river import metrics
from river import compose
from river import optim
from river import preprocessing
from river import datasets
from river import stream
from river import time_series
from river import drift

#metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
from sklearn.metrics import pairwise_distances, pairwise_kernels
from sklearn.utils.validation import check_array, check_consistent_length, _num_samples
from pmdarima.metrics import smape

from sklearn.model_selection import KFold

#visualization
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()

import seaborn as sns

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

from sklearn.metrics.pairwise import paired_distances, pairwise_distances, PAIRWISE_DISTANCE_FUNCTIONS
from scipy.spatial import distance
from scipy import stats

from tslearn import metrics as tsd  #ctw, dtw, soft_dtw, lcss, gak, sigma_gak
from tslearn.neighbors import KNeighborsTimeSeries

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import VotingRegressor

#from tqdm import tqdm
from tqdm.notebook import tqdm, tqdm_notebook, tnrange
from tqdm import trange

"""# Descrição do Algoritmo

Descrição 

1. Seleciona a série (país) e transforma para dados diários
2. Retira outliers (percentil 99%)
3. Delimita final da série: 31/12/2021
4. Train/val/test split
  - train: 70%
  - val: 20%
  - test: 10%

TREINAMENTO
5. Aplica detector de drift e define as janelas a partir dos pontos de drift (usando a série diária, com trend não acha drift, e acumulada acha demais)
6. Treina um modelo para cada janela 
  a. SARIMA com filtro khalmann até 7 lags
  b. auto-arima para definir ordem 

SELEÇÃO
7. Com os dados prévios ao da previsão (novos dados), aplica-se a decomposição sazonal e usa o componente de tendência para extrair os parâmetros da reta(intercepto e coeficiente angular)
8. Compara o vetor de parâmetros da regressão dos novos dados com os respectivos de cada janela obtida durante o treinamento, usando a distância pelo cosseno.
9. Seleciona o modelo da janela com menor distância em relação aos novos dados.

PREVISÃO
10. Usa esse modelo para fazer a previsão, de forma dinâmica, ou seja, recursiva, utilizando os dados da própria previsão para prever os próximos valores da série
11. Repete a seleção para cada novo intervalo de dados a serem previstos (semana, quinzena etc)

# Functions
"""

def get_slope(y, x):
    X = x
    X = sm.add_constant(X)
    mod = sm.OLS(y, X, hasconst=True)
    res = mod.fit()
    return res.params['const'], res.params['x1']

def compare_slope_old(intercept1, slope1, intercept2, slope2):
  return distance.cosine([intercept1, slope1], [intercept2, slope2])

def compare_slope(intercept1, slope1, intercept2, slope2):
  r1 = [intercept1 + slope1*n for n in range(1000)]
  r2 = [intercept2 + slope2*n for n in range(1000)]
  return distance.cosine(r1,r2)

def regfunc(intp,slp):
  return [ intp + slp*n for n in range(100)]

r1 = regfunc(2,1)
r2 = regfunc(4,1)

distance.cosine(r1,r2)

"""Load data"""

#cases
source = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')
#deaths
source2 = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')
#brazil
source3 = pd.read_csv('https://raw.githubusercontent.com/wcota/covid19br/master/cases-brazil-states.csv', parse_dates=True)

data = source.loc[source['Province/State'].isna()]
df = pd.DataFrame(data.T[4:-1])
df.columns = data['Country/Region']
df = df.astype('int32')
df['total'] = df.T.sum()

countries = df.columns

results = {}

"""## Preprocessing

Data cleaning
"""

#df.rename({'Country/Region':'date'},axis=1)

#df.to_csv('/content/drive/MyDrive/Dissertação/codrift-19/2/data/global.csv')

#df['Brazil'].diff().fillna(0).to_csv('/content/drive/MyDrive/Dissertação/codrift-19/2/data/brazil_daily.csv')

"""
for country in tqdm(countries):
  print(country)
  df[country].diff().fillna(0).to_csv('/content/drive/MyDrive/Dissertação/codrift-19/2/data/'+country+'_daily.csv')
"""

#FH = 30

end = dt(year=2021, month=12, day=31)

df.index = pd.to_datetime(df.index)
df2 = df.loc[pd.date_range(start=end, end='2022-02-20')]

df.index = pd.to_datetime(df.index)
df = df.loc[pd.date_range(start='2020-01-22', end=end)]
#df = df[:434].set_index(pd.date_range(start='2020-01-22', end=end))

country_selection = widgets.Dropdown(
    options=countries,
    value=None,
    description='Country:',
    disabled=False,
)
#country_selection

country_selection

#country = 'Brazil'
country = 'US'
#country = country_selection.value

def fix_outliers(df,q = .99, zero = True):
  # cortar período inicial sem casos
  for k,d in enumerate(df):
    if k > 0 and d > df.iloc[k-1]:
      break
  df = df.iloc[k:]
  # substituir zeros
  if zero == False:
    df = df.mask(df == 0, np.nan).fillna(method='bfill')
  # converter valores extremos para NaN e substituir pelo anterior
  df = df.mask(df > df.quantile(q), np.nan).fillna(method='bfill')
  df = df.mask(df < 0, np.nan).fillna(0)
  return df

data = fix_outliers(df[country].diff().fillna(0))

data2 = fix_outliers(df2[country].diff().fillna(0))

train_data = data.iloc[:int(len(data)*0.7)] 
val_data = data.iloc[int(len(data)*0.7):int(len(data)*0.9)]
test_data = data.iloc[-int(len(data)*0.1):]
test_data2 = data2
results = {}
results['val_data'] = {}
results['test_data'] = {}
results_series = {}
results_series['val_data'] = {}
results_series['val_data']['val_data'] = val_data
results_series['test_data'] = {}
results_series['test_data']['test_data'] = test_data
print(train_data.shape, val_data.shape, test_data.shape, )

def set_results(true, pred, model, test='val_data'):
  results[test][model] = {}
  results[test][model]['mae'] = mean_absolute_error(true,pred)
  results[test][model]['rmse'] = mean_squared_error(true,pred, squared=False)
  results[test][model]['r2'] = r2_score(true,pred)
  results[test][model]['mape'] = mean_absolute_percentage_error(true,pred)
  return True

def base_plots(df):
  fig, [ax1,ax2] = plt.subplots(1,2, figsize=(20,8))
  ax1.plot(df.index, df)
  ax2.plot(df.index, df.cumsum())
  for tick in ax1.get_xticklabels():
        tick.set_rotation(90)
  for tick in ax2.get_xticklabels():
      tick.set_rotation(90)

  #plt.xticks(rotation=90)
base_plots(data)

decomp = tsa.seasonal_decompose(data)
decomp.plot().set_size_inches((20, 10));

fig, ax = plt.subplots(2,1,figsize=(30,6))
train_data.plot(ax=ax[0])
#train_data.rolling(7).mean().plot()
#train_data.rolling(7, win_type="triang").mean().plot(ax=ax[0])
#train_data.rolling(7, win_type="exponential").mean().plot()
train_data.rolling(7, win_type="flattop").mean().plot(ax=ax[0])
train_data.plot(ax=ax[1])
train_data.rolling(7, win_type="parzen").mean().plot(ax=ax[1])
#train_data.rolling(7, win_type="gaussian").mean(std=0.5).plot()

"""# Drift Detection

Drift detection to create time windows with differents patterns

## KSWIN
"""

#data_diff = train_data.fillna(0).to_dict()
data_diff = tsa.seasonal_decompose(train_data.diff().fillna(0)).trend.to_dict()

adwin = drift.KSWIN(alpha=0.02, window_size=60)
detector='kswin'

drifts =[]
for k in data_diff:
  in_drift, in_warning = adwin.update(data_diff[k])
  if in_drift:
    drifts.append(k) 
    #print(f"Change detected at index {k}, input value: {data[k]}")  
drifts

drift_data = {}
drift_index = []
for key, drift_point in enumerate(drifts):
  #print(key)
  #print(len(drifts))
  #print(drift_point)
  if key == 0:
    drift_data[key] = train_data[:drift_point]
    #drift_index.extend(np.repeat(key,len(train_data[:drift_point])))
  elif key == len(drifts)-1:
    #end = df.index[-60]
    drift_data[key] = train_data[drifts[key-1]:drift_point]
    #drift_index.extend(np.repeat(key,len(train_data[drifts[key-1]:drift_point])))
    drift_data[key+1] = train_data[drift_point:]
    #drift_index.extend(np.repeat(key+1,len(train_data[drift_point:])))
    #drift_data[key+1] = train_data[drift_point:end]
  else:
    drift_data[key] = train_data[drifts[key-1]:drift_point]
    #drift_index.extend(np.repeat(key,len(train_data[drifts[key-1]:drift_point])))
drift_data

fig, (ax1, ax2) = plt.subplots(2,1,figsize=(18,8), sharex=True)
for d in range(len(drift_data)):
  ax1.plot(pd.Series(drift_data[d]).fillna(method='bfill'))
ax1.plot(val_data.fillna(method='bfill'), color='darkgray', linestyle='--' )

ax1.bar(x=drifts, height=train_data.fillna(method='bfill').loc[drifts], width=2, color='r')
for k, drift_point in enumerate(drifts):
    #ax1.annotate(k, xy=(10, 100),  xycoords='axes points', xytext=(drift_point, -10), textcoords='data')
    #ax1.annotate(drift_point.date().strftime('%Y-%m-%d'), xy=(10, -100),  xycoords='axes points', xytext=(drift_point-delta(days=10), -150), textcoords='data', rotation=90)
    ax1.annotate(drift_point.date().strftime('%m/%y'), xy=(10, -100),  xycoords='axes points', xytext=(drift_point-timedelta(days=10), -150), textcoords='data')
    ax1.annotate(k, xy=(10, -5000),  xycoords='axes points', xytext=(drift_point-timedelta(days=20), -5500), textcoords='data')
ax2.plot(train_data.append(val_data).cumsum(), color='darkgray', linestyle='--' )
ax2.plot(train_data.cumsum())

try:
    #os.makekdirs('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/')
    os.makedirs('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector)
    plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/drifts.png')
    pd.DataFrame(drift_data).to_excel('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/drift_data.xlsx')
except:
    plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/drifts.png')
    pd.DataFrame(drift_data).to_excel('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/drift_data.xlsx')

#for d in range(len(drift_data)):
  #ax3.boxplot(pd.Series(drift_data[d]).fillna(method='bfill').box())
#ax3.plot(test_data.fillna(method='bfill').diff(), color='gray')
#pd.concat([pd.DataFrame(drift_data),test_data]).plot.box(figsize=(18,4))
pd.concat([pd.DataFrame(drift_data),val_data],axis=1).plot.box(figsize=(18,4))
#ax3.plot.boxplot(pd.DataFrame(drift_data))
#ax2.bar(x=drifts, height=test_data.loc[drifts], width=2, color='r')
plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/boxplot.png')

"""## ADWIN

Learning from Time-Changing Data with Adaptive Windowing ∗

https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.2279&rep=rep1&type=pdf

https://riverml.xyz/latest/api/drift/ADWIN/
"""

data_diff = train_data.to_dict()
#data_diff = tsa.seasonal_decompose(train_data.diff().fillna(0)).trend.to_dict()
#tsa.seasonal_decompose(train_data.diff().fillna(0)).trend.plot(figsize=(20,5))
#pd.DataFrame.from_dict(data_diff, orient='index').plot()
#future = df[country][-FH:]

adwin = drift.ADWIN()
detector = 'adwin'

drifts =[]
for k in data_diff:
  in_drift, in_warning = adwin.update(data_diff[k])
  if in_drift:
    drifts.append(k) 
    #print(f"Change detected at index {k}, input value: {data[k]}")  
drifts

def get_drifts(train_data, window):
  data_diff = train_data.to_dict()
  adwin = drift.ADWIN(window)
  detector = 'adwin'
  drifts =[]
  for k in data_diff:
    in_drift, in_warning = adwin.update(data_diff[k])
    if in_drift:
      drifts.append(k) 
  return drifts

"""## Drifts"""

def get_drift_data(drifts):
  drift_data = {}
  drift_index = []
  for key, drift_point in enumerate(drifts):
    #print(key)
    #print(len(drifts))
    #print(drift_point)
    if key == 0:
      drift_data[key] = train_data[:drift_point]
      #drift_index.extend(np.repeat(key,len(train_data[:drift_point])))
    elif key == len(drifts)-1:
      #end = df.index[-60]
      drift_data[key] = train_data[drifts[key-1]:drift_point]
      #drift_index.extend(np.repeat(key,len(train_data[drifts[key-1]:drift_point])))
      drift_data[key+1] = train_data[drift_point:]
      #drift_index.extend(np.repeat(key+1,len(train_data[drift_point:])))
      #drift_data[key+1] = train_data[drift_point:end]
    else:
      drift_data[key] = train_data[drifts[key-1]:drift_point]
      #drift_index.extend(np.repeat(key,len(train_data[drifts[key-1]:drift_point])))
  return drift_data, drift_index
drift_data, drift_index = get_drift_data(drifts)

def draw_drifts(drifts ,drift_data, train_data):
  fig, (ax1, ax2) = plt.subplots(2,1,figsize=(18,8), sharex=True)
  for d in range(len(drift_data)):
    ax1.plot(pd.Series(drift_data[d]).fillna(method='bfill'))
  ax1.plot(val_data.fillna(method='bfill'), color='darkgray', linestyle='--' )

  ax1.bar(x=drifts, height=train_data.fillna(method='bfill').loc[drifts].values, width=2, color='r')
  for k, drift_point in enumerate(drifts):
      #ax1.annotate(k, xy=(10, 100),  xycoords='axes points', xytext=(drift_point, -10), textcoords='data')
      #ax1.annotate(drift_point.date().strftime('%Y-%m-%d'), xy=(10, -100),  xycoords='axes points', xytext=(drift_point-delta(days=10), -150), textcoords='data', rotation=90)
      ax1.annotate(drift_point.date().strftime('%m/%y'), xy=(10, -100),  xycoords='axes points', xytext=(drift_point-timedelta(days=10), -150), textcoords='data')
      ax1.annotate(k, xy=(10, -5000),  xycoords='axes points', xytext=(drift_point-timedelta(days=20), -5500), textcoords='data')
  ax2.plot(train_data.append(val_data).cumsum(), color='darkgray', linestyle='--' )
  ax2.plot(train_data.cumsum())

draw_drifts(drifts,drift_data, train_data)

for w in [0.001, 0.002, 0.005, 0.01]:
  #print(w)
  dfts = get_drifts(train_data, w)
  #print(dfts)
  ddfts, ddftsi = get_drift_data(dfts)
  #print(len(ddfts))
  draw_drifts(dfts, ddfts, train_data)

drifts = get_drifts(train_data, 0.001)
  drift_data, drift_index = get_drift_data(drifts)

"""
try:
    #os.makekdirs('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/')
    os.makedirs('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector)
    plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/drifts.png')
    pd.DataFrame(drift_data).to_excel('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/drift_data.xlsx')
except:
    plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/drifts.png')
    pd.DataFrame(drift_data).to_excel('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/drift_data.xlsx')
#for d in range(len(drift_data)):
  #ax3.boxplot(pd.Series(drift_data[d]).fillna(method='bfill').box())
#ax3.plot(test_data.fillna(method='bfill').diff(), color='gray')
#pd.concat([pd.DataFrame(drift_data),test_data]).plot.box(figsize=(18,4))
pd.concat([pd.DataFrame(drift_data),val_data],axis=1).plot.box(figsize=(18,4))
#ax3.plot.boxplot(pd.DataFrame(drift_data))
#ax2.bar(x=drifts, height=test_data.loc[drifts], width=2, color='r')
plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/boxplot.png')
"""

fig, axs = plt.subplots(2,5, figsize=(20,10))
for i in range(len(drift_data)):  
  decomp = tsa.seasonal_decompose(drift_data[i])
  if i < 5 :
    axs[0, i].plot(decomp.observed)
    axs[0, i].set_xticklabels([])
  else:
    axs[1, i-5].plot(decomp.observed)
    axs[1, i-5].set_xticklabels([])
#plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/windows.png')

#dir(plot_acf(drift_data[i], lags = 14, label = "90"))

fig, axs = plt.subplots(2,5, figsize=(20,10))
for i in range(len(drift_data)):  
  decomp = tsa.seasonal_decompose(drift_data[i])
  if i < 5 :
    axs[0, i].plot(decomp.trend)
    axs[0, i].set_xticklabels([])    
  else:
    axs[1, i-5].plot(decomp.trend)
    axs[1, i-5].set_xticklabels([])
#plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/windows_trend.png')

df_drift = pd.DataFrame()
axs = {}
eqs = {}
X = 0
for key, drift_point in enumerate(drifts):
  tmp = pd.DataFrame(drift_data[key])
  tmp['pattern'] = key
  df_drift = df_drift.append(tmp)
  X = pd.Series(drift_data[key].reset_index(drop=True)).index
  X = sm.add_constant(X)
  y = pd.Series(drift_data[key])
  mod = sm.OLS(y, X, hasconst=True)
  res = mod.fit()
  #print(res.params)
  eqs[key] = [res.params['x1'], res.params['const'], ]
#tmp = pd.DataFrame(drift_data[key+1])
#tmp['pattern'] = key+1
df_drift = df_drift.append(tmp)
frame = df_drift.reset_index().reset_index()
g = sns.lmplot(data=frame, x='level_0', y=country, col='pattern', col_wrap=5, fit_reg=True, scatter=True, truncate=True, legend=True, sharex=False, sharey=False)
#g = sns.FacetGrid(frame, col='pattern', col_wrap=4,)
#g.map_dataframe(sns.regplot, x="level_0", y="Brazil")
#g.figure.subplots_adjust(wspace=0, hspace=0)
for k, ax in g.axes_dict.items():
  begin = int(frame.loc[frame['pattern'] == k].index[0])
  end = int(frame.loc[frame['pattern'] == k].index[-1])
  ax.set_xbound(begin, end)
  ax.annotate('y= {:+.1f}x {:+.1f} '.format(eqs[k][0],eqs[k][1]), xy=(0, 0),  xycoords='axes points', xytext=(0,0), textcoords='offset points')
    
plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/drifts/'+detector+'/windows_slope.png')

distance.cosine([0,2], [0,1])

sns.regplot(val_data.reset_index().index, val_data)

new_params = {}
new_distances = {}
for week in range(1):
    #new_data = val_data.iloc[week*7:(week+1)*7]
    new_data = val_data
    X = pd.Series(new_data.reset_index(drop=True)).index
    X = sm.add_constant(X)
    y = pd.Series(new_data)
    mod = sm.OLS(y, X, hasconst=True)
    res = mod.fit()
    #print(res.params['const'])
    #print(res.params['x1'])
    new_params[week] = [res.params['x1'], res.params['const']]
    print(new_params[week])
    new_distances[week] = {}
    for k, eq in eqs.items():
      print(k)
      print(eq)
      d = distance.cosine([eq[1],eq[0]],[new_params[week][1],new_params[week][0]])
      print(d)
      new_distances[week][k] = d

"""::"""

print((pd.Series(new_distances[0])).abs().idxmin())
(pd.Series(new_distances[0])).abs()

for key, drift_point in enumerate(drifts):
    X = pd.Series(drift_data[key].reset_index(drop=True)).index
    X = sm.add_constant(X)
    y = pd.Series(drift_data[key])
    mod = sm.OLS(y, X, hasconst=True)
    res = mod.fit()
    print(res.params['const'])
    print(res.params['x1'])

def get_slope(y, x):
    X = x
    X = sm.add_constant(X)
    mod = sm.OLS(y, X, hasconst=True)
    res = mod.fit()
    return res.params['const'], res.params['x1']

def compare_slope(intercept1, slope1, intercept2, slope2):
  return distance.cosine([intercept1, slope1],[intercept2, slope2])

fig, axs = plt.subplots(2,5, figsize=(20,10))
for i in range(10):  
  decomp = tsa.seasonal_decompose(drift_data[i])
  if i < 5 :
    axs[0, i].acorr(drift_data[i], maxlags = 14, normed=False)
    axs[0, i].set_xlim(left=0)        
    axs[0, i].set_title(i)    
  else:
    axs[1, i-5].acorr(drift_data[i], maxlags = 14, normed=False)
    axs[1, i-5].set_title(i) 
    axs[1, i-5].set_xlim(left=0)
    #axs[1, i-5].set_xticklabels([])

fig, axs = plt.subplots(2,5, figsize=(20,10))
for i in range(10):  
  decomp = tsa.seasonal_decompose(drift_data[i])
  if i < 5 :
    axs[0, i].plot(decomp.seasonal)
    axs[0, i].set_xticklabels([])
  else:
    axs[1, i-5].plot(decomp.seasonal)
    axs[1, i-5].set_xticklabels([])

"""## Windows comps"""

for k in drift_data.keys():
  print(len(drift_data[k]))

"""### Drifts Normalization"""

drift_data_norm = {}
for k in drift_data.keys():
  drift_data_norm[k] = (drift_data[k] - drift_data[k].mean()) / drift_data[k].std()

drift_array = np.zeros((len(drift_data), 100))
print(drift_array.shape)
for k in drift_data.keys():
  tmp = np.array(pd.Series(drift_data_norm[k]).reset_index(drop=True))
  drift_array[k, :len(tmp)] = tmp

"""# Ensembles

## Sarimas
"""

drift_data.keys()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# autoarima = {}
# models = {}
# predictions = {}
# val_metrics = {}
# for d in trange(len(drift_data.keys())):
#   print('\n')
#   print(d)
#   time_len = len(drift_data[d])
#   #train_drift_data = drift_data[d]
#   #test_drift_data = drift_data[d][int(time_len*0.8):]
#   #print(drift_data[d])
#   try:
#     autoarima[d] = pmautoarima(drift_data[d], max_p=7, d=2, max_q=7, m=7, trace=True, information_criterion='aic', seasonal=True, maxiter = 50, stepwise=True)
#     #print(autoarima[d].summary())
#     models[d] = SARIMA(drift_data[d].astype(float), order=autoarima[d].order, seasonal_order=autoarima[d].seasonal_order).fit()
#   except:
#     pass
#

try:
  os.makedirs('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/train/')
except:
  pass

for k, model in models.items():
  report = model.summary().as_text()
  path ='/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/train/'+str(k)+'.txt'
  with open(path, 'w') as f:
    f.write(report)

for k, model in models.items():
    fig, ax = plt.subplots()
    drift_data[k].plot()
    plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/train/'+str(k)+'.jpg')
    model.plot_diagnostics(figsize=(10,8))
    try:
        os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/train/')
        plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/train/'+str(k))
    except:
        plt.savefig('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/train/'+str(k))

"""### Update data"""

models_applied = {}
for k, m in models.items():
  models_applied[k] = m.apply(val_data)

models_applied_test = {}
for k, m in models.items():
  models_applied_test[k] = m.apply(test_data)

"""### Save"""

#os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/base/')
#os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/update/')

for k, m in models.items():
    try:
        os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/base/')
        m.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/base/'+str(k))
    except:
        m.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/base/'+str(k))

for k, m in models_applied.items():
    try:
        m.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/update/'+str(k))
    except:
        os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/update/')
        m.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/update/'+str(k))

for k, m in models_applied_test.items():
    try:
        m.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/update_test/sarima_'+str(k))
    except:
        os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/update_test/')
        m.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/update_test/sarima_'+str(k))

"""## Load Sarimas

"""

list_models = sorted(os.listdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/base/'))
models = {}
for k in list_models:
    models[int(k)] = sm.load('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/base/'+str(k))

list_models = sorted(os.listdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/update/'))
models_updated = {}
for k in list_models:
    models_updated[int(k)] = sm.load('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/update/'+str(k))

list_models = sorted(os.listdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/update_test/'))
models_updated_test = {}
for k, m in enumerate(list_models):
    models_updated_test[int(k)] = sm.load('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/update_test/'+str(m))

"""### One"""


def get_selection(new_data, models, size, k):
  


drift_data.pop(10)

def get_predictions(pool):
  predictions_base = {}
  for k, m in pool.items():
    predictions_base[k] = m.predict()
  return predictions_base

def get_selection_window(data, size):
  return data.rolling(window=size)

def comp_windows(new_data, old_data, filter_size):
    #selection_window = pd.Series(new_data).rolling(filter_size).mean().dropna()
    selection_window = pd.Series(new_data).ewm(com=0.5, adjust=True).mean()
    selection_window_index = selection_window.index
    #print(selection_window, selection_window_index)
    new_slope = get_slope(selection_window, selection_window_index)
    
    #old_window = pd.Series(old_data).rolling(filter_size).mean().dropna()
    old_window = pd.Series(old_data).ewm(com=0.5, adjust=True).mean()
    old_window_index = selection_window.index
    old_slope = get_slope(old_window, old_window_index)

    return compare_slope_old(old_slope[0], old_slope[1], new_slope[0], new_slope[1])

def comp_windows2(new_data, old_data):
    return tsd.dtw(new_data, old_data)

def get_comps(new_data, drift_data, filter_size):
  comps = {}
  for sw in get_selection_window(new_data, 14):
    if len(sw) == 14:
      comps[sw.index[-1]] = {}
      for ix, dwi in drift_data.items():
        comps[sw.index[-1]][ix] = []
        for ow in get_selection_window(dwi, 14):
          if len(ow) == 14:
            #print(sw.values, ow.values)
            comps[sw.index[-1]][ix].append(comp_windows(sw.values, ow.values, filter_size))
            #comps[sw.index[-1]][ix].append(comp_windows2(sw.values, ow.values))
        comps[sw.index[-1]][ix] = np.array(comps[sw.index[-1]][ix]).min()
  return comps

# Commented out IPython magic to ensure Python compatibility.
# %%time
# comps = get_comps(val_data, drift_data)

def model_selection(comps, pool, k=1):
  selection_predictions = {}
  preds = pd.DataFrame(get_predictions(pool))
  if k == 1:
    for tup in pd.DataFrame(comps).dropna(axis=1).idxmin(axis=0).items():
      #print(tup[0])
      #print(tup[1])
      #print(preds.at[tup[0], tup[1]])
      selection_predictions[tup[0]] = preds.at[tup[0], tup[1]]
  return selection_predictions

y_pred = pd.Series(model_selection(comps, models_updated))
#pd.Series(val_data).plot()
mean_absolute_error(y_pred,val_data.reindex(y_pred.index))

for fs in [5,7,9,11,13]:
  print(fs)
  comps = get_comps(val_data, drift_data, fs)
  y_pred = pd.Series(model_selection(comps, models_updated))
  #pd.Series(val_data).plot()
  print(mean_absolute_error(y_pred,val_data.reindex(y_pred.index)))

fig, ax = plt.subplots(figsize=(20,5))
y_pred.plot(label='sel')
val_data.plot(label='val')
plt.legend()

mean_absolute_percentage_error(y_pred,val_data.reindex(y_pred.index))

y_pred = pd.Series(model_selection(comps, models_updated))
#pd.Series(val_data).plot()
mean_absolute_error(y_pred,val_data.reindex(y_pred.index))

fig, ax = plt.subplots(figsize=(20,5))
y_pred.plot(label='sel')
val_data.plot(label='val')
plt.legend()

mean_absolute_percentage_error(y_pred,val_data.reindex(y_pred.index))

def get_oracle(predictions, true):
  df_error = predictions.sub(true, axis=0).abs()
  oracle = {}
  for row in df_error.rank(axis=1).idxmin(axis=1).items():
    #print(row)
    oracle[row[0]] = predictions.at[row[0], row[1]]
  return pd.Series(oracle)

mean_absolute_error(get_oracle(pd.DataFrame(get_predictions(models_updated)), val_data), val_data)

set_results(val_data.iloc[1:], get_oracle(pd.DataFrame(get_predictions(models_updated)), val_data).iloc[1:], 'oracle', test='val_data')

mean_absolute_percentage_error(get_oracle(pd.DataFrame(get_predictions(models_updated)), val_data), val_data)

pd.DataFrame(comps).dropna(axis=1).idxmin(axis=0)

pd.DataFrame(get_predictions(models_updated))

pd.DataFrame(comps).dropna(axis=1).idxmin(axis=0)

for day, value in get_selection_window.items():
  comps[day] = {}
  selection_window = data.loc[day - selection_size * timedelta(days=1):day]
  selection_window_index = list(range(len(selection_window)))
  new_slope = get_slope(selection_window, selection_window_index)
  for k, m in drift_data.items():
    if k < len(drift_data)-1:
      old_slope = (get_slope(m, list(range(len(m))) ))
      #print(k, old_slope)
      comp = compare_slope_old(old_slope[0], old_slope[1], new_slope[0], new_slope[1])
      #print('d', comp)
      comps[day][k] = comp

"""#### compare_slope_old"""

selection_size=14

predictions_base = {}
for k, m in models_updated.items():
  predictions_base[k] = m.predict()

comps = {}
for day, value in val_data.items():
  comps[day] = {}
  selection_window = data.loc[day - selection_size * timedelta(days=1):day]
  selection_window_index = list(range(len(selection_window)))
  new_slope = get_slope(selection_window, selection_window_index)
  for k, m in drift_data.items():
    if k < len(drift_data)-1:
      old_slope = (get_slope(m, list(range(len(m))) ))
      #print(k, old_slope)
      comp = compare_slope_old(old_slope[0], old_slope[1], new_slope[0], new_slope[1])
      #print('d', comp)
      comps[day][k] = comp

comps1 = comps.copy()

predictions_selection = {}
selections_models = []
for day, model in pd.DataFrame(comps).idxmin().items():
  print(day)
  print(model)
  selections_models.append(model)
  predictions_selection[day.strftime('%Y-%m-%d')] = predictions_base[model][day.strftime('%Y-%m-%d')]

set_results(val_data.iloc[1:], pd.Series(predictions_selection).iloc[1:], 'selection_k1_old', test='val_data')

results_series['val_data']['selection_k1_old'] = pd.Series(predictions_selection)

"""#### test"""

predictions_base = {}
for k, m in models_updated_test.items():
  predictions_base[k] = m.predict()

comps = {}
for day, value in test_data.items():
  comps[day] = {}
  selection_window = data.loc[day - selection_size * timedelta(days=1):day]
  selection_window_index = list(range(len(selection_window)))
  new_slope = get_slope(selection_window, selection_window_index)
  for k, m in drift_data.items():
    if k < len(drift_data)-1:
      old_slope = (get_slope(m, list(range(len(m))) ))
      #print(k, old_slope)
      comp = compare_slope_old(old_slope[0], old_slope[1], new_slope[0], new_slope[1])
      #print('d', comp)
      comps[day][k] = comp

predictions_selection = {}
selections_models = []
for day, model in pd.DataFrame(comps).idxmin().items():
  selections_models.append(model)
  predictions_selection[day] = predictions_base[model][day]

set_results(test_data, pd.Series(predictions_selection), 'selection_k1_old', test='test_data')

"""### compare_slope_new"""

predictions_base = {}
for k, m in models_updated.items():
  predictions_base[k] = m.predict()

comps = {}
for day, value in val_data.items():
  comps[day] = {}
  selection_window = data.loc[day - selection_size * timedelta(days=1):day]
  selection_window_index = list(range(len(selection_window)))
  new_slope = get_slope(selection_window, selection_window_index)
  for k, m in drift_data.items():
    if k < len(drift_data)-1:
      old_slope = (get_slope(m, list(range(len(m))) ))
      #print(k, old_slope)
      comp = compare_slope(old_slope[0], old_slope[1], new_slope[0], new_slope[1])
      #print('d', comp)
      comps[day][k] = comp

comps2 = comps.copy()

comps1 == comps2

predictions_selection = {}
selections_models = []
for day, model in pd.DataFrame(comps).idxmin().items():
  selections_models.append(model)
  predictions_selection[day] = predictions_base[model][day]

set_results(val_data.iloc[1:], pd.Series(predictions_selection).iloc[1:], 'selection_k1_new', test='val_data')

results_series['val_data']['selection_k1_new'] = pd.Series(predictions_selection)

"""#### test"""

predictions_base = {}
for k, m in models_updated_test.items():
  predictions_base[k] = m.predict()

comps = {}
for day, value in test_data.items():
  comps[day] = {}
  selection_window = data.loc[day - selection_size * timedelta(days=1):day]
  selection_window_index = list(range(len(selection_window)))
  new_slope = get_slope(selection_window, selection_window_index)
  for k, m in drift_data.items():
    if k < len(drift_data)-1:
      old_slope = (get_slope(m, list(range(len(m))) ))
      #print(k, old_slope)
      comp = compare_slope_old(old_slope[0], old_slope[1], new_slope[0], new_slope[1])
      #print('d', comp)
      comps[day][k] = comp

predictions_selection = {}
selections_models = []
for day, model in pd.DataFrame(comps).idxmin().items():
  selections_models.append(model)
  predictions_selection[day] = predictions_base[model][day]

set_results(test_data, pd.Series(predictions_selection), 'selection_k1_new', test='test_data')

"""#### K = 3"""

models_full = {}
for k, m in models.items():
  models_full[k] = m.apply(data)

kn = len(models_full)

errors = {}
bk = {}
for tup in pd.DataFrame(comps).rank().T.iterrows():
  t = tup[0]
  #print(t)
  day = t
  near_models = tup[1].loc[tup[1] < kn+1].index
  print(near_models)
  errors[t] = {}
  selection_window = data.loc[day - selection_size * timedelta(days=1):day]
  for i, mdi in enumerate(near_models):
    #models_updated[mdi].predict(start=days*t, end=days*(t+1), dynamic=False)
    last_pred = models_full[mdi].predict(start=selection_window.index[0], \
                                         end=selection_window.index[14])
    last_truth = selection_window
    lmae = mean_absolute_error(last_truth, last_pred)
    errors[t][mdi] = lmae
    #errors[t][mdi] = mean_squared_error(ground_truth, y_pred)
  bk[day] = pd.Series(errors[t]).idxmin()

predictions_selection = {}
selections_models = []
for day, model in bk.items():
  selections_models.append(model)
  predictions_selection[day] = predictions_base[model][day]

set_results(val_data.iloc[1:], pd.Series(predictions_selection).iloc[1:], 'selection_k3_new', test='val_data')

results_series['val_data']['selection_k3_new'] = pd.Series(predictions_selection)

pd.DataFrame(results['val_data'])

"""## 14 dias

#### trend
"""

predictions = {}
true = {}
trend = {}
days = 14
fig, ax = plt.subplots()
sns.regplot(val_data, val_data.reset_index().index)
fig, ax = plt.subplots(2,5, figsize=(30,6))

for week in trange(0,9):
  predictions[week] = {}
  true[week] = val_data.iloc[week*days:((week+1)*days)+1]
  trend[week] = tsa.seasonal_decompose(val_data.iloc[week*days:((week+1)*days)+1]).trend.dropna()
  if week//5 > 0 :
    sns.regplot(list(range(len(trend[week]))) , trend[week], ax=ax[week//5,week-5])
    ax[week//5,week - 5].set_title(week-1)
    
  else :
    sns.regplot(list(range(len(trend[week]))) , trend[week], ax=ax[week//5,week-1])
    ax[week//5,week-1].set_title(week-1)
    
  for k, m in models_updated.items():
    #print(week)
    #print(true[week])
    #print(k)
    predictions[week][k] = m.predict(start=week*days, end=((week+1)*days), dynamic=True)
    #predictions[week][k] = m.predict(start=week*days, end=((week+1)*days)-1, dynamic=False)
    #print(predictions[week][k])  
    #print('-' * 100)

"""#### cosine"""

comps = {}

comps = {}
for week in range(1, 9):
  new_slope = get_slope(trend[week-1], list(range(len(trend[week-1]))))
  #print('new', new_slope)
  comps[week] = {}
  for k, m in drift_data.items():
    old_slope = (get_slope(m, list(range(len(m))) ))
    #print(k, old_slope)
    comp = compare_slope(old_slope[0], old_slope[1], new_slope[0], new_slope[1])
    #print('d', comp)
    comps[week][k] = comp

comps

"""#### neighbors/error"""

kn = 3

errors = {}
bk = []
for tup in pd.DataFrame(comps).rank().T.iterrows():
  t = tup[0]
  print(t)
  near_models = tup[1].loc[tup[1] < kn+1].index
  #print(near_models)
  errors[t] = {}
  fig, ax = plt.subplots(1,3, figsize=(20,6))
  plt.xticks(rotation = 90)
  for i, mdi in enumerate(near_models):
    #models_updated[mdi].predict(start=days*t, end=days*(t+1), dynamic=False)
    last_pred = models_updated[mdi].predict(start=days*(t-1), end=days*(t), dynamic=False)
    y_pred = models_updated[mdi].predict(start=days*t, end=days*(t+1), dynamic=False)
    #ground_truth = val_data.iloc[days*t:days*(t+1)+1]
    ground_truth = true[t]
    ground_trend = trend[t]
    last_truth = true[t-1]
    last_trend = trend[t-1]
    lmae = mean_absolute_error(last_truth, last_pred)
    mae = mean_absolute_error(ground_truth, y_pred)
    print(lmae)
    ax[i].plot(ground_truth, label='true', color='black', linestyle='--')
    ax[i].plot(ground_trend, label='trend', color='gray', linestyle='--')
    ax[i].plot(last_truth, label='last true')
    ax[i].plot(last_trend, label='last trend')
    ax[i].plot(last_pred, label='last pred')
    ax[i].plot(y_pred, label=mdi)
    ax[i].bar(x=true[t].index[0], height=pd.concat([last_truth,ground_truth], axis=0).max(), width=0.5, color='r')
    ax[i].legend()
    errors[t][mdi] = lmae
    ax[i].annotate('LAST MAE: '+ '{:,.2f}'.format(lmae), xy=(0, 0),  xycoords='axes points', xytext=(0,0), textcoords='offset points')
    ax[i].annotate('MAE: '+ '{:,.2f}'.format(mae), xy=(0, 10),  xycoords='axes points', xytext=(0,10), textcoords='offset points')
    
    #errors[t][mdi] = mean_squared_error(ground_truth, y_pred)
  bk.append(pd.Series(errors[t]).idxmin())
  plt.title('best: '+str(pd.Series(errors[t]).idxmin()))
bk

bk

MAES = {}
for week in tnrange(1,9):
  MAES[week] = {}
  fig, ax = plt.subplots(1,len(predictions[week])+1, figsize=(36,4))
  for k, m in predictions[week].items():
    MAES[week][k] = {}
    ax[k].plot(true[week], label='true')
    ax[k].plot(trend[week], label='trend')
    ax[k].plot(predictions[week][k], label='pred')
    ax[k].set_xlabel('val_week_'+str(week))
    ax[k].set_label(['true', 'pred'])
    ax[k].set_title('sarima_'+str(k))
    ax[k].axes.xaxis.set_ticklabels([])
    mse = mean_squared_error(true[week], predictions[week][k], squared=False)
    mae = mean_absolute_error(true[week], predictions[week][k])
    MAES[week][k] = mae
    ax[k].annotate('RMSE: '+ '{:,.2f}'.format(mse), xy=(0, 0),  xycoords='axes points', xytext=(0,0), textcoords='offset points')
    ax[k].annotate('MAE: '+ '{:,.2f}'.format(mae), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
    ax[k].legend()
  ax[k+1].plot(val_data,  color='gray', linestyle='--', label='full')
  ax[k+1].plot(val_data.iloc[week*7:(week+1)*7],  color='red', linestyle='-', label='new')
  ax[k+1].set_xlabel('val_week_'+str(week))
  ax[k+1].axes.xaxis.set_ticklabels([])
  ax[k+1].legend()

for week in range(1, 9):
  fig, ax = plt.subplots()
  sns.regplot(true[week-1].reset_index(drop=true).index, true[week-1].reset_index(drop=true))
  sns.regplot(drift_data[pd.Series(comps[week]).idxmin()].reset_index(drop=true).index, drift_data[pd.Series(comps[week]).idxmin()].reset_index(drop=true))
  plt.title(pd.Series(comps[week]).idxmin())

pd.DataFrame(comps)

pd.DataFrame(comps)

pd.DataFrame(comps).corr()

predictions_selection = {}
selections_models = []
for week, model in pd.DataFrame(comps).idxmin(axis=0).items():
  selections_models.append(model)
  predictions_selection[week] = predictions[week][model]

pd.DataFrame(comps)

#new selection
predictions_selection = {}
selections_models = []
for t, m in enumerate(bk):
  selections_models.append(m)
  predictions_selection[t+1] = predictions[t+1][m]

pd.DataFrame(predictions[6]).join(pd.DataFrame(true[6])).plot(backend='plotly')

pd.DataFrame(predictions[7]).plot()
plt.legend()

pd.DataFrame(predictions_selection).plot()

len(predictions_selection)

#ensemble
#predictions_selection = {}
#for week, model in pd.DataFrame(comps).idxmin(axis=0).items():
#  selections_models.append(model)
#  predictions_selection[week] = predictions[week][model]

MAE = {}
MSE = {}
oracle_week = []
oracle_week_models = []
for week in trange(1, 9):
  MAE[week] = {}
  MSE[week] = {}
  for k, m in predictions[week].items():
    #print(k)
    #print(m.values)
    MAE[week][k] = mean_absolute_error(m.values, true[week])
    MSE[week][k] = mean_squared_error(m.values, true[week])
  best = pd.Series(MAE[week]).idxmin() 
  oracle_week_models.append(best)
  oracle_week.append(predictions[week][best])
  MAE[week]['oracle_week'] = mean_absolute_error(predictions[week][best], true[week])
  MSE[week]['oracle_week'] = mean_squared_error(predictions[week][best], true[week])
  MAE[week]['selection'] = mean_absolute_error(predictions_selection[week], true[week])
  MSE[week]['selection'] = mean_squared_error(predictions_selection[week], true[week])
pd.DataFrame(MAE).min(axis=0).mean()

pd.DataFrame(MAE)[:len(models_updated)].mean(axis=1)

best_wmodel = pd.DataFrame(MAE)[:len(models_updated)].mean(axis=1).idxmin()
best_wmodel

pd.DataFrame(MAE).rank(axis=0).mean(axis=1).apply(lambda x: 1/x).plot.bar().set_title('Model Mean Rank')

preds4 = pd.concat([v for week in predictions.values() for k, v in week.items() if k == best_wmodel ])

selections_models



pd.concat(predictions_selection.values()).plot(label='selection', color='red', linestyle='--')

fig, ax = plt.subplots(figsize=(30,6))
preds4.plot(label='model4')
val_data.plot(label='true')
pd.concat(oracle_week).plot(label='oracle', color='black', linestyle='-.')
pd.concat(predictions_selection.values()).plot(label='selection', color='red', linestyle='--')
plt.legend()

pd.DataFrame(MAE).loc['selection'].mean()

pd.DataFrame(MAE).iloc[:-2].mean(axis=1).mean()

fig, ax = plt.subplots(2,2 ,figsize=(30,10))
ax[0,0].plot(val_data, label='true')
mae_all = pd.DataFrame(MAE).iloc[:-2].mean(axis=1).mean()
ax[0,0].annotate('MAE ALL MODELS: '+ '{:,.2f}'.format(mae_all), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0,0].legend()

ax[0,1].plot(val_data, label='true')
ax[0,1].plot(preds4, label='model4')
mae_4 =  pd.DataFrame(MAE).loc[best_wmodel].mean()
ax[0,1].annotate('MAE: '+ '{:,.2f}'.format(mae_4), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0,1].legend()

ax[1,0].plot(val_data, label='true')
mae_sel =  pd.DataFrame(MAE).loc['selection'].mean()
ax[1,0].plot(pd.concat(predictions_selection.values()), label='selection')
ax[1,0].annotate('MAE: '+ '{:,.2f}'.format(mae_sel), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1,0].legend()

ax[1,1].plot(val_data, label='true')
ax[1,1].plot(pd.concat(oracle_week), label='oracle_week')
mae_oracle =  pd.DataFrame(MAE).loc['oracle_week'].mean()
ax[1,1].annotate('MAE: '+ '{:,.2f}'.format(mae_oracle), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1,1].legend()

#pd.concat(oracle_week).plot(label='oracle')
#pd.Series(predictions_selection[week]).plot(label='selection')
#plt.legend()

pd.DataFrame({'selection': selections_models, 'oracle': oracle_week_models}).plot.bar(xlabel='quinzena', ylabel='modelo' )

#pd.DataFrame({'val': val_data, 'model4': preds4.mask(preds4 < 0,0), 'oracle_week':pd.concat(oracle_week), 'selection': pd.Series(predictions_selection[week])}).dropna().plot(figsize=(20,10), subplots=True)

pd.DataFrame(MAE).rank()

print('Week Winner\n', pd.DataFrame(MAE).rank(axis=0).idxmin())
print('Model WinCounts\n',pd.DataFrame(MAE).rank(axis=0).idxmin().value_counts())

print('erro por modelo')
#print(pd.DataFrame(MAE).mean(axis=1))
pd.DataFrame(MAE).rank(axis=0).mean(axis=1).plot.bar(label='mean')
plt.legend()

pd.DataFrame(MAE).mean(axis=0).plot(label='mean')
pd.DataFrame(MAE).min(axis=0).plot(label='min')
plt.title('Week Error')
plt.legend()

def oracle(predictions, true):
  errors = pd.DataFrame(predictions).sub(true, axis=0).abs()

predictions[1]

pd.DataFrame(predictions[1]).sub(true[1], axis=0).abs().astype('int64')

pd.DataFrame(predictions[1]).sub(true[1], axis=0).abs().rank(ascending=False)

"""## Voting Regressor"""

def shift_data(data, lags):
  data_list = list()
  for t in range(-lags,0):
    data_list.append(data.shift(t))
    df = pd.concat(data_list, axis=1)
  #print(df.columns)
  #print(list(range(-lags,0)))
  df.columns = list(range(-lags,0))
  return df

X = shift_data(train_data,7).dropna()
y = train_data.reindex(X.index)

reg1 = GradientBoostingRegressor(random_state=1)
reg2 = RandomForestRegressor(random_state=1)
reg3 = LinearRegression()
ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])
ereg = ereg.fit(X, y)

X_val = shift_data(val_data,7).dropna()
y_val = val_data.reindex(X_val.index)

preds = ereg.predict(X_val)

plt.plot(preds, label='pred')
plt.plot(y_val.reset_index(drop=True), label='true')
plt.legend()

print("{:,.2f}".format(mean_absolute_error(preds, y_val)))
print("{:,.2f}".format(mean_squared_error(preds, y_val)))

X_test = shift_data(test_data,7).dropna()
y_test = test_data.reindex(X_test.index)

predst = ereg.predict(X_test)

plt.plot(predst, label='pred')
plt.plot(y_test.reset_index(drop=True), label='true')
plt.legend()

print("{:,.2f}".format(mean_absolute_error(predst, y_test)))
print("{:,.2f}".format(mean_squared_error(predst, y_test)))

"""# Benchmark

Benchmark models using classical models

## Last Values (one step)
"""

lv_val = val_data.shift(1).dropna()
#lv_val[-1] = lv_val[-2]
lv_val

plt.figure(figsize=(20,6))
plt.plot(val_data, label='val_data')
plt.plot(lv_val, label='lv')
plt.legend()

#lv = train_data[-30:]
lv_test = test_data.shift(1).dropna()
lv_test

plt.figure(figsize=(20,6))
plt.plot(test_data, label='val_data')
plt.plot(lv_test, label='lv')
plt.legend()

def set_results(true, pred, model, test='val_data'):
  results[test][model] = {}
  results[test][model]['mae'] = mean_absolute_error(true,pred)
  results[test][model]['rmse'] = mean_squared_error(true,pred, squared=False)
  results[test][model]['r2'] = r2_score(true,pred)
  results[test][model]['mape'] = mean_absolute_percentage_error(true,pred)
  return True

set_results(val_data.iloc[1:], lv_val, 'lv', 'val_data')

results_series['val_data']['lv'] = lv_val

set_results(test_data.iloc[1:], lv_test, 'lv', 'test_data')

results

try:

except:
    os.makedirs('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/')

"""## Arima"""

arima_order = pmautoarima(train_data, max_p=7, d=1, max_q=7, m=7, seasonal=False, trace=True, information_criterion='aic', suppress_warnings=True, maxiter = 50, stepwise=True)

arima_base = ARIMA(train_data, order=arima_order.order)

arima_model = arima_base.fit()

arima_model.plot_diagnostics(figsize=(10,10));

try:
  os.makedirs('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/')
except:
  pass

report = arima_model.summary().as_text()
path ='/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/summary.txt'
with open(path, 'w') as f:
  f.write(report)

arima_updated = arima_model.apply(val_data)

arima_updated_test = arima_model.apply(test_data)

"""### Save"""

try:
    os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/')
    arima_model.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/arima_base')
except:
    arima_model.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/arima_base')

try:
    arima_updated.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/arima_update')
except:
    os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/')
    arima_updated.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/arima_update')

try:
    arima_updated_test.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/arima_update_test')
except:
    os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/')
    arima_updated_test.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/arima_update_test')

"""### Load"""

arima_updated = sm.load('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/arima_update')

arima_updated_test = sm.load('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/arima/arima_update_test')

arima_one_val = arima_updated.predict()
arima_one_test = arima_updated_test.predict()
set_results(val_data.iloc[1:], arima_one_val.iloc[1:], 'arimao', 'val_data')
set_results(test_data.iloc[1:], arima_one_test.iloc[1:], 'arimao', 'test_data')

results_series['val_data']['arimao'] = arima_one_val.iloc[1:]

arima_predictions_multi = {}
arima_predictions_one = {}
days = 30
true = {}

for week in trange(1,7):
  arima_predictions_multi[week] = {}
  true[week] = val_data.iloc[week*days:(week+1)*days]
  arima_predictions_multi[week] = arima_updated.predict(start=week*days, end=((week+1)*days)-1, dynamic=True)
  arima_predictions_one[week] = arima_updated.predict(start=week*days, end=((week+1)*days)-1, dynamic=False)
  fig, ax = plt.subplots()
  ax.plot(arima_predictions_multi[week], label='arima_multi')
  ax.plot(arima_predictions_one[week], label='arima_one')
  ax.plot(true[week], label='true')
  ax.legend()
  plt.xticks(rotation = 90)

#marks = [pred.index[0] for pred in arima_predictions.values()]

for week in arima_predictions_multi.keys():
  try:
    MAE[week]['arimao_multi'] = mean_absolute_error(arima_predictions_multi[week], true[week])
    MSE[week]['arimao_multi'] = mean_squared_error(arima_predictions_multi[week], true[week])
    MAE[week]['arimao_one'] = mean_absolute_error(arima_predictions_one[week], true[week])
    MSE[week]['arimao_one'] = mean_squared_error(arima_predictions_one[week], true[week])
  except:
    pass

fig, ax = plt.subplots(2,2 ,figsize=(30,10))
ax[0,0].plot(val_data, label='true')
ax[0,0].plot(pd.concat(arima_predictions_multi.values()), label='arimao_multi')
mae_sar =  pd.DataFrame(MAE).loc['arimao_multi'].mean()
mse_sar =  pd.DataFrame(MSE).loc['arimao_multi'].mean()
#mae_all = pd.DataFrame(MAE).iloc[:-2].mean(axis=1).mean()
ax[0,0].annotate('MAE: '+ '{:,.2f}'.format(mae_sar), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0,0].annotate('MSE: '+ '{:,.2f}'.format(mse_sar), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
#ax[0,0].annotate('MAE ALL MODELS: '+ '{:,.2f}'.format(mae_all), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0,0].set_xlabel('14 steps ahead')
ax[0,0].legend()

ax[0,1].plot(val_data, label='true')
ax[0,1].plot(pd.concat(arima_predictions_one.values()), label='arimao_one')
mae_sar =  pd.DataFrame(MAE).loc['arimao_one'].mean()
mse_sar =  pd.DataFrame(MSE).loc['arimao_one'].mean()
#mae_all = pd.DataFrame(MAE).iloc[:-2].mean(axis=1).mean()
ax[0,1].annotate('MAE: '+ '{:,.2f}'.format(mae_sar), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0,1].annotate('MSE: '+ '{:,.2f}'.format(mse_sar), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
#ax[0,0].annotate('MAE ALL MODELS: '+ '{:,.2f}'.format(mae_all), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0,1].set_xlabel('14 steps ahead')
ax[0,1].legend()

"""
ax[1,0].plot(val_data, label='true')
mae_sel =  pd.DataFrame(MAE).loc['selection'].mean()
mse_sel =  pd.DataFrame(MSE).loc['selection'].mean()
ax[1,0].plot(pd.concat(predictions_selection.values()), label='selection')
ax[1,0].annotate('MAE: '+ '{:,.2f}'.format(mae_sel), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1,0].annotate('MSE: '+ '{:,.2f}'.format(mse_sel), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
ax[1,0].legend()

ax[1,1].plot(val_data, label='true')
ax[1,1].plot(pd.concat(oracle_week), label='oracle_week')
mae_oracle =  pd.DataFrame(MAE).loc['oracle_week'].mean()
mse_oracle =  pd.DataFrame(MSE).loc['oracle_week'].mean()
ax[1,1].annotate('MAE: '+ '{:,.2f}'.format(mae_oracle), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1,1].annotate('MSE: '+ '{:,.2f}'.format(mse_oracle), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
ax[1,1].legend()
ax[0,0].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[0,1].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[1,0].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[1,1].bar(x=marks, height=val_data.max(), width=0.6, color='r')
"""

fig, ax = plt.subplots(1,2 ,figsize=(30,10))
ax[0].plot(val_data, label='true')
ax[0].plot(pd.concat(arima_predictions_multi.values()), label='arimao_multi')
mae_sar =  pd.DataFrame(MAE).loc['arimao_multi'].mean()
mse_sar =  pd.DataFrame(MSE).loc['arimao_multi'].mean()
#mae_all = pd.DataFrame(MAE).iloc[:-2].mean(axis=1).mean()
ax[0].annotate('MAE: '+ '{:,.2f}'.format(mae_sar), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0].annotate('MSE: '+ '{:,.2f}'.format(mse_sar), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
#ax[0,0].annotate('MAE ALL MODELS: '+ '{:,.2f}'.format(mae_all), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0].set_xlabel('14 steps ahead')
ax[0].legend()

ax[1].plot(val_data, label='true')
ax[1].plot(pd.concat(arima_predictions_one.values()), label='arimao_one')
mae_sar =  pd.DataFrame(MAE).loc['arimao_one'].mean()
mse_sar =  pd.DataFrame(MSE).loc['arimao_one'].mean()
#mae_all = pd.DataFrame(MAE).iloc[:-2].mean(axis=1).mean()
ax[1].annotate('MAE: '+ '{:,.2f}'.format(mae_sar), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1].annotate('MSE: '+ '{:,.2f}'.format(mse_sar), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
#ax[0,0].annotate('MAE ALL MODELS: '+ '{:,.2f}'.format(mae_all), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1].set_xlabel('14 steps ahead')
ax[1].legend()

"""
ax[1,0].plot(val_data, label='true')
mae_sel =  pd.DataFrame(MAE).loc['selection'].mean()
mse_sel =  pd.DataFrame(MSE).loc['selection'].mean()
ax[1,0].plot(pd.concat(predictions_selection.values()), label='selection')
ax[1,0].annotate('MAE: '+ '{:,.2f}'.format(mae_sel), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1,0].annotate('MSE: '+ '{:,.2f}'.format(mse_sel), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
ax[1,0].legend()

ax[1,1].plot(val_data, label='true')
ax[1,1].plot(pd.concat(oracle_week), label='oracle_week')
mae_oracle =  pd.DataFrame(MAE).loc['oracle_week'].mean()
mse_oracle =  pd.DataFrame(MSE).loc['oracle_week'].mean()
ax[1,1].annotate('MAE: '+ '{:,.2f}'.format(mae_oracle), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1,1].annotate('MSE: '+ '{:,.2f}'.format(mse_oracle), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
ax[1,1].legend()
ax[0,0].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[0,1].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[1,0].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[1,1].bar(x=marks, height=val_data.max(), width=0.6, color='r')
"""

fig, ax = plt.subplots(1,2 ,figsize=(30,10))
ax[0].plot(val_data, label='true')
ax[0].plot(pd.concat(arima_predictions_multi.values()), label='arimao_multi')
mae_sar =  pd.DataFrame(MAE).loc['arimao_multi'].mean()
mse_sar =  pd.DataFrame(MSE).loc['arimao_multi'].mean()
#mae_all = pd.DataFrame(MAE).iloc[:-2].mean(axis=1).mean()
ax[0].annotate('MAE: '+ '{:,.2f}'.format(mae_sar), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0].annotate('MSE: '+ '{:,.2f}'.format(mse_sar), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
#ax[0,0].annotate('MAE ALL MODELS: '+ '{:,.2f}'.format(mae_all), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0].set_xlabel('30 steps ahead')
ax[0].legend()

ax[1].plot(val_data, label='true')
ax[1].plot(pd.concat(arima_predictions_one.values()), label='arimao_one')
mae_sar =  pd.DataFrame(MAE).loc['arimao_one'].mean()
mse_sar =  pd.DataFrame(MSE).loc['arimao_one'].mean()
#mae_all = pd.DataFrame(MAE).iloc[:-2].mean(axis=1).mean()
ax[1].annotate('MAE: '+ '{:,.2f}'.format(mae_sar), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1].annotate('MSE: '+ '{:,.2f}'.format(mse_sar), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
#ax[0,0].annotate('MAE ALL MODELS: '+ '{:,.2f}'.format(mae_all), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1].set_xlabel('30 steps ahead')
ax[1].legend()

"""
ax[1,0].plot(val_data, label='true')
mae_sel =  pd.DataFrame(MAE).loc['selection'].mean()
mse_sel =  pd.DataFrame(MSE).loc['selection'].mean()
ax[1,0].plot(pd.concat(predictions_selection.values()), label='selection')
ax[1,0].annotate('MAE: '+ '{:,.2f}'.format(mae_sel), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1,0].annotate('MSE: '+ '{:,.2f}'.format(mse_sel), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
ax[1,0].legend()

ax[1,1].plot(val_data, label='true')
ax[1,1].plot(pd.concat(oracle_week), label='oracle_week')
mae_oracle =  pd.DataFrame(MAE).loc['oracle_week'].mean()
mse_oracle =  pd.DataFrame(MSE).loc['oracle_week'].mean()
ax[1,1].annotate('MAE: '+ '{:,.2f}'.format(mae_oracle), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1,1].annotate('MSE: '+ '{:,.2f}'.format(mse_oracle), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
ax[1,1].legend()
ax[0,0].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[0,1].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[1,0].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[1,1].bar(x=marks, height=val_data.max(), width=0.6, color='r')
"""

"""## Sarima"""

sarima_order = pmautoarima(train_data, max_p=7, d=1, max_q=7, m=7, seasonal=True, trace=True, information_criterion='aic', suppress_warnings=True, maxiter = 50, stepwise=True)

sarima_base = SARIMA(train_data, order=sarima_order.order, seasonal_order=sarima_order.seasonal_order)

sarima_model = sarima_base.fit()

sarima_model.plot_diagnostics(figsize=(10,10));

try:
  os.makedirs('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/')
except:
  pass

report = sarima_model.summary().as_text()
path ='/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/summary.txt'
with open(path, 'w') as f:
  f.write(report)

sarima_updated = sarima_model.apply(val_data)

sarima_updated_test = sarima_model.apply(test_data)

"""#### Save"""

try:
    os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/')
    sarima_model.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/sarima_base')
except:
    sarima_model.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/sarima_base')

try:
    sarima_updated.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/sarima_update')
except:
    os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/')
    sarima_updated.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/sarima_update')

try:
    sarima_updated_test.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/sarima_update_test')
except:
    os.mkdir('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/')
    sarima_updated_test.save('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/sarima_update_test')

"""#### Load"""

sarima_updated = sm.load('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/sarima_update')

sarima_updated_test = sm.load('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/sarima/sarima_update_test')

sarima_one_val = sarima_updated.predict()
sarima_one_test = sarima_updated_test.predict()
set_results(val_data.iloc[1:], sarima_one_val.iloc[1:], 'sarimao', 'val_data')
set_results(test_data.iloc[1:], sarima_one_test.iloc[1:], 'sarimao', 'test_data')

pd.DataFrame(results['val_data'])

results_series['val_data']['sarimao'] = sarima_one_val.iloc[1:]

sarima_predictions = {}
sarima_predictions_stat = {}
days = 30
true = {}

for week in trange(1,5):
  sarima_predictions[week] = {}
  true[week] = val_data.iloc[week*days:(week+1)*days]
  sarima_predictions[week] = sarima_updated.predict(start=week*days, end=((week+1)*days)-1, dynamic=True)
  sarima_predictions_stat[week] = sarima_updated.predict(start=week*days, end=((week+1)*days)-1, dynamic=False)
  fig, ax = plt.subplots()
  ax.plot(sarima_predictions[week], label='sarima_multi')
  ax.plot(sarima_predictions_stat[week], label='sarima_one')
  ax.plot(true[week], label='true')
  ax.legend()
  plt.xticks(rotation = 90)

for week in sarima_predictions.keys():
  MAE[week]['sarimao'] = mean_absolute_error(sarima_predictions[week], true[week])
  MSE[week]['sarimao'] = mean_squared_error(sarima_predictions[week], true[week])

marks = [pred.index[0] for pred in sarima_predictions.values()]

fig, ax = plt.subplots(2,2 ,figsize=(30,10))
ax[0,0].plot(val_data, label='true')
ax[0,0].plot(pd.concat(sarima_predictions.values()), label='sarimao')
mae_sar =  pd.DataFrame(MAE).loc['sarimao'].mean()
mse_sar =  pd.DataFrame(MSE).loc['sarimao'].mean()
#mae_all = pd.DataFrame(MAE).iloc[:-2].mean(axis=1).mean()
ax[0,0].annotate('MAE: '+ '{:,.2f}'.format(mae_sar), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0,0].annotate('MSE: '+ '{:,.2f}'.format(mse_sar), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
#ax[0,0].annotate('MAE ALL MODELS: '+ '{:,.2f}'.format(mae_all), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0,0].legend()

ax[0,1].plot(val_data, label='true')
ax[0,1].plot(preds4, label='model4')
mae_4 =  pd.DataFrame(MAE).loc[4].mean()
mse_4 =  pd.DataFrame(MSE).loc[4].mean()
ax[0,1].annotate('MAE: '+ '{:,.2f}'.format(mae_4), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[0,1].annotate('SE: '+ '{:,.2f}'.format(mse_4), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
ax[0,1].legend()

ax[1,0].plot(val_data, label='true')
mae_sel =  pd.DataFrame(MAE).loc['selection'].mean()
mse_sel =  pd.DataFrame(MSE).loc['selection'].mean()
ax[1,0].plot(pd.concat(predictions_selection.values()), label='selection')
ax[1,0].annotate('MAE: '+ '{:,.2f}'.format(mae_sel), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1,0].annotate('MSE: '+ '{:,.2f}'.format(mse_sel), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
ax[1,0].legend()

ax[1,1].plot(val_data, label='true')
ax[1,1].plot(pd.concat(oracle_week), label='oracle_week')
mae_oracle =  pd.DataFrame(MAE).loc['oracle_week'].mean()
mse_oracle =  pd.DataFrame(MSE).loc['oracle_week'].mean()
ax[1,1].annotate('MAE: '+ '{:,.2f}'.format(mae_oracle), xy=(0, 10), xycoords='axes points', xytext=(0,10), textcoords='offset points')
ax[1,1].annotate('MSE: '+ '{:,.2f}'.format(mse_oracle), xy=(0, 0), xycoords='axes points', xytext=(0,0), textcoords='offset points')
ax[1,1].legend()
ax[0,0].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[0,1].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[1,0].bar(x=marks, height=val_data.max(), width=0.6, color='r')
ax[1,1].bar(x=marks, height=val_data.max(), width=0.6, color='r')

pd.DataFrame(MAE).loc[['oracle_week', 'selection', 'sarimao']].T.plot.bar()

sarima_predictions = sarima_model.predict(val_data)

sarima_results = pd.DataFrame({'true':test_data, 'pred':sarima_predictions, 'diff': test_data - sarima_predictions})

print ('MAE: ', mean_absolute_error(test_data,sarima_predictions))
print ('RMSE: ', mean_squared_error(test_data,sarima_predictions, squared=False))
print ('SMAPE: ', smape(test_data,sarima_predictions))
print ('MAPE: ', mean_absolute_error(test_data,sarima_predictions)*100/ test_data.mean())

sarima_results[['true','pred']].plot()

plot_acf(sarima_results['diff'], lags = 14, label = "90");
plot_pacf(sarima_results['diff'], lags = 14, label = "90");

"""## SVR"""

from sklearn import svm
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor

X_train = pd.DataFrame(tsa.tsatools.add_lag(train_data, lags=6)).iloc[:-1]

y_train = train_data.iloc[7:]

X_test = pd.DataFrame(tsa.tsatools.add_lag(val_data, lags=6)).iloc[:-1]

y_test = val_data.iloc[7:]

svrs = {}
for k in ['linear', 'poly', 'rbf', 'sigmoid']:
  svrs[k] = {}
  for d in [2,3,4,5]:
    svrs[k][d] = {}
    regr = svm.SVR(kernel=k, degree=d)
    svrs[k][d] = regr.fit(X_train, y_train)

svrs = {}
for k in ['linear', 'poly', 'rbf', 'sigmoid']:
  svrs[k] = {}
  regr = svm.SVR(kernel=k)
  svrs[k] = regr.fit(X_train, y_train)

for k in ['linear', 'poly', 'rbf', 'sigmoid']:
  for d in [2,3,4,5]:
    print(k,d)
    mean_absolute_error(svrs[k][d].predict(X_test), y_test)

regr = svm.SVR(kernel='linear')

regr.fit(X_train, y_train)
mean_absolute_error(regr.predict(X_test), y_test)

mean_absolute_error(regr.predict(X_test), y_test)

regr = svm.SVR(kernel='sigmoid')
regr.fit(X_train, y_train)
mean_absolute_error(regr.predict(X_test), y_test)

regr = svm.SVR(kernel='poly', degree=7)
regr.fit(X_train, y_train)
mean_absolute_error(regr.predict(X_test), y_test)

regr = svm.SVR(kernel='poly', gamma='auto')
regr.fit(X_train, y_train)
mean_absolute_error(regr.predict(X_test), y_test)

neigh = KNeighborsRegressor(n_neighbors=3, metric='chebyshev')

neigh.fit(X_train, y_train)
mean_absolute_error(neigh.predict(X_test), y_test)

regr = RandomForestRegressor(max_depth=5, random_state=0, n_estimators=1000)

regr.fit(X_train, y_train)
mean_absolute_error(regr.predict(X_test), y_test)

pd.Series(regr.predict(X_test)).plot()
#y_test.plot()

"""## Pytorch"""

import torch
from torch import nn
import torch.nn.functional as F

from skorch import NeuralNetRegressor

from sklearn.datasets import make_regression

class RegressorModule(nn.Module):
    def __init__(
            self,
            num_units=10,
            nonlin=F.relu,
    ):
        super(RegressorModule, self).__init__()
        self.num_units = num_units
        self.nonlin = nonlin

        self.dense0 = nn.Linear(20, num_units)
        self.nonlin = nonlin
        self.dense1 = nn.Linear(num_units, 10)
        self.output = nn.Linear(10, 1)

    def forward(self, X, **kwargs):
        X = self.nonlin(self.dense0(X))
        X = F.relu(self.dense1(X))
        X = self.output(X)
        return X

net_regr = NeuralNetRegressor(
    RegressorModule,
    max_epochs=20,
    lr=0.1,
#     device='cuda',  # uncomment this to train with CUDA
)

X_regr, y_regr = make_regression(1000, 20, n_informative=10, random_state=0)
X_regr = X_regr.astype(np.float32)
y_regr = y_regr.astype(np.float32) / 100
y_regr = y_regr.reshape(-1, 1)

print(X_regr.shape, y_regr.shape)

net_regr.fit(X_regr, y_regr)

class RegressorModule2(nn.Module):
    def __init__(
            self,
            num_units=10,
            nonlin=F.relu,
    ):
        super(RegressorModule2, self).__init__()
        self.num_units = num_units
        self.nonlin = nonlin

        self.dense0 = nn.Linear(7, num_units)
        self.nonlin = nonlin
        self.dense1 = nn.Linear(num_units, 10)
        self.output = nn.Linear(10, 1)

    def forward(self, X, **kwargs):
        X = self.nonlin(self.dense0(X))
        X = F.relu(self.dense1(X))
        X = self.output(X)
        return X

net_regr2 = NeuralNetRegressor(
    RegressorModule2,
    max_epochs=20,
    lr=0.1,
#     device='cuda',  # uncomment this to train with CUDA
)

X_regr.shape

np.array(X_train).shape

y_regr.shape

np.array(y_train).reshape(-1,1).shape

X_train = np.array(X_train.astype('float32'))
X_train = X_train / X_train.max()
y_train = np.array(y_train.astype('float32')).reshape(-1,1)

net_regr2.fit(np.array(X_train.astype('float32'))  , np.array(y_train.astype('float32')).reshape(-1,1))

y_train.shape

net_regr.fit(X_train, np.array(y_train).reshape(-1,1))

"""# Evaluation"""

pd.DataFrame(results['val_data']).rename({'lv':'random_walk', 'selection_k3_new':'selection_all', 'selection_k1_old' : 'selection_k1'},axis=1).style.format('{:.2f}')

pd.DataFrame(results['val_data']).to_excel('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/val_metrics.xlsx')

pd.DataFrame(results['test_data']).to_excel('/content/drive/MyDrive/Dissertação/codrift-19/2/models/'+country+'/benchmark/test_metrics.xlsx')

pd.DataFrame(results_series['val_data']).plot(subplots=True, figsize=(20,5), layout=(4,2))

